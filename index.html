<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Personal Projects</title>
    <link rel="stylesheet" href="styles.css">
</head>
<body>
    <header>
        <section class="hero">
            <h1>Personal Projects - Arian Torshizi</h1>
            <p>A description of each of the projects can be found below as well as links to the code and screenshots.</p>
            <a target="_blank" href="https://github.com/atorshizi/Personal_Projects/tree/main/" target="_blank">View GitHub</a>
        </section>
    </header>
    <main>
        <div class="vertical-menu">
            <details>
                <summary>Tarot Card Reader</summary>
                <p>TO BE ADDED</p> 
                <p>The main code can be found at the below link: 
                <br> 
                <a target="_blank" href="https://github.com/atorshizi/Personal_Projects/Tarot Card Reader/source">Source Code</a> 
                </p> 
            </details> 
            
            <details>
                <summary>PantryPal</summary>
                <p>This project is a desktop app that can generate and store recipes for food. Recipes are generated by recording the user saying the type of meal they would like (Breakfast, Lunch, Dinner, etc.) and the ingredients they have available to use for the meal. The voice recordings are then transcribed using the Whisper API and then prompt ChatGPT's API for a suggested meal with instruction of how to make the meal. The DALL-E API is also called to generate a sample image of what the meal could look like. The generated recipe for the user is then stored on a database (on MongoDB) and can be viewed, editted, or deleted later on another device that the user logs into. </p> 
                <p float="left">
                  <img src="Screenshots/login.png" alt="screenshot of app first loaded" style="width:250px;"> 
                  <img src="Screenshots/create.png" alt="screenshot taking a picture" style="width:250px;"> 
                  <img src="Screenshots/home.png" alt="screenshot taking a picture" style="width:250px;"> 
                </p> 
                <p>Above we can see the login screen for the app where the user can log in with their credentials or create a new account by giving a username and password. Then, once logged in, the user is shown the home screen (picture 3) where they can access previous saved recipes or add a new recipe. With the "remember me" feature in the log in screen, the user can bypass logging in in the future on the current device. </p>

                <p float="left">
                  <img src="Screenshots/rec.png" alt="screenshot of app first loaded" style="width:250px;"> 
                  <img src="Screenshots/recing.png" alt="screenshot taking a picture" style="width:250px;"> 
                  <img src="Screenshots/gen.png" alt="screenshot taking a picture" style="width:400px;"> 
                </p>
                <p>Here we can see the process for creating a new recipe. The user must first record the type of meal and the ingredients they want to use. Then, they are shown a confirmation page where they can manually or choose to have ChatGPT and DALL-E find a new recipe and picture with the same ingredients. </p>

                <p float="left">
                  <img src="Screenshots/filt.png" alt="screenshot taking a picture" style="width:400px;"> 
                  <img src="Screenshots/det.png" alt="screenshot taking a picture" style="width:400px;"> 
                </p>
                
                <p>Above, we see how the user car can access previously saved recipes. A list of recipes are shown and can be sorted (by name or time of generation) and/or filtered (by meal type). By clicking on a recipe we can access a detailed page of the ingredients, instructions, and image. </p>
                <p>The main code can be found at the below link: 
                <br> 
                <a target="_blank" href="https://github.com/atorshizi/Personal_Projects/tree/79628b268bd6d09c629dfae177248366346acb40/PantryPal/src">Source Code</a> 
                </p> 
            </details> 

            <details>
            <summary>OCR</summary>
            <p>In this project, I created an android app that can recognize text in images from either existing images or from new images that can be taken from the camera directly from the app - this is a form of Optical Character Recognition (OCR). Further, it can also scan one or multiple barcode/QR codes from a single picture and display the raw data and format type - it supports 13 different barcode/QR code formats. It accomplishes these tasks by incorporating the Google ML Kit SDK. 

              <p float="left">
                <img src="Screenshots/Screenshot_20220909-014106_OCR.png" alt="screenshot of app first loaded" style="width:200px;">
                <img src="Screenshots/Screenshot_20220909-014226_Camera.png" alt="screenshot taking a picture" style="width:200px;">
                <img src="Screenshots/Screenshot_20220909-014237_OCR.png" alt="screenshot of processed image" style="width:200px;">
                <img src="Screenshots/Screenshot_20220909-014800_Camera.png" alt="screenshot of taking a picture" style="width:200px;">
                <img src="Screenshots/Screenshot_20220909-014652_OCR.png" alt="screenshot of processed QR" style="width:200px;">
                <img src="Screenshots/Screenshot_20220909-014540_OCR.png" alt="screenshot of processed barcode" style="width:200px;">
                </p>
                
              <p>Above, we can see the startup page of the app, as well as a demo in which the user takes a picture a sentence in the app and the sentence is then processed and shown below the image taken in the app in text form (image 3). It also shows a demo in which a single barcode/QR code is scanned and the respective value is then given to the user. </p>                  
              <p>
                The main code and apk can be found at the below links:
                <br> 
                <a target="_blank" href="https://github.com/atorshizi/Personal_Projects/blob/main/OCR/app/src/main/java/com/example/ocr/MainActivity.java">MainActivity.java</a> 
                <br> 
                <a target="_blank" href="https://github.com/atorshizi/Personal_Projects/blob/main/OCR/app/src/main/res/layout/activity_main.xml">MainActivity.xml</a> 
                <br> 
                <a target="_blank" href="https://github.com/atorshizi/Personal_Projects/blob/main/OCR/app/OCR.apk">apk file</a> 
              </p> 
            </details>

            <details>
                <summary>Custom ASIC + Encoder and Decoder</summary>
              <p>The goal of the project was to design a custom application-specific integrated circuit (ASIC) for a viterbi encoder and decoder on a 9-bit system. The instruction set architecture (ISA) was first designed with the a total of 8 instructions available taking up 3 bits and the other bits depended on the type of instruction. The ISA is summerized below.</p>
              <p float="left"> 
                <img src="Custom ASIC/isa.png" style="width:400px;">
                <img src="Custom ASIC/op1.png" style="width:400px;">
                <img src="Custom ASIC/op2.png" style="width:400px;">
                <img src="Custom ASIC/op3.png" style="width:400px;">
                <img src="Custom ASIC/op4.png" style="width:400px;"> 
              </p> 
              <p>Next, a custom assembler was created in pyton that translated any assembly code writen in the custom ISA to machine code that was loaded into the instruction ROM to be ran. The machine modules were coded in SystemVerilog and simulated in ModelSim and synthesized in Quartus. An overview of the machine and of one of the modules (the ALU) can be seen along with a brief section of the decoder written in the custom ISA</p>
              <p float="left"> 
                <img src="Custom ASIC/overview.png" style="width:auto;">
                <img src="Custom ASIC/alu.png" style="width:700px;">
                <img src="Custom ASIC/decoder-example.png" style="width:130px;"> 
              </p> 
            </details> 

            <details>
                <summary>Face Recognition</summary>
              <p>TO BE ADDED</p>
            </details> 

            <details>
              <summary>Email Spam Filter</summary>
              <p>This project makes use of Tensorflow to train a machine learning algorithm that can classify inputed emails based on if they are spam emails or not - it uses a neural network with two hidden layers to do so along with a bag-of-words vectorization. Two files are provided to train the model and to test the accuracy - TrainModel.py separates the full dataset such that roughly 80% is used for training and 20% for testing to be done in TestModel.py. In testing, it achieved an accuracy of over 96%.</p>
              <p float="left">
                <img src="Screenshots/CL1.png" alt="an example a loss vs epoch graph during training" style="width:325px;">
                <img src="Screenshots/CL2.png" alt="Screenshot of achieved validation accuracy" style="width:300px;">
                <img src="Screenshots/CL3.png" alt="Screenshot of training in progress" style="width:400px;">
              </p>
              
              <p>Shown in the picture is an example loss vs epoch graph taken during training in the first image. In the second image, we can see the accuracy that was achieved with the dataset that the algorithm was not trained on. In the third image, we have an example screenshot showing the program training in progress. </p>

              <p>The main code can be found at the below link: 
                <br> 
                <a target="_blank" href="https://github.com/atorshizi/Personal_Projects/blob/main/Classification">Source Code</a> 
              </p> 
          </details> 

          <details>
                <summary>Gradient Descent Implementation for Regression</summary>
              <p> 
              This is an implementation of a regression program - a form of supervised machine learning - that can handle datasets with multiple features. There are two main files within it: Custom_BGD_Regression and SKL_Regression. The custom implementation minimizes the squared-error cost function using batch gradient descent. It processes a .csv file to use as the training data and allows users to change the default values of the learning rate, maximum number of iterations of the gradient descent, and the epsilon value that is used to determine the convergence of the gradient descent. Once trained, the developed model can be used to predict values for other examples. Using matplotlib, plots of the learning curve and individual features can be shown. 
              The second file, SKL_Regression, is a roughly functionally equivalent program using stochastic gradient descent which was developed using scikit_learn specifically, the SGDRegressor. Further specifications on program methods and .csv formatting are commented in the program files. There is an included tester file which is used to test various methods of both programs.
              </p>

              <p float="left">
                <img src="Screenshots/Figure_1.png" alt="learning curve example" style="width:350px;">
                <img src="Screenshots/Figure_2.png" alt="feature plot example" style="width:350px;">
                <img src="Screenshots/Figure_3.png" alt="feature plot example" style="width:375px;">
                <img src="Screenshots/Figure_4.png" alt="command line prediction example" style="width:375px;">
              </p>
              <p>The screenshots show an example of a learning curve that was given using the BGD program in the first screenshot as well as plots of different datasets' features vs. the true output as given in the training data - one set has 3 features and the other has 18 - to be used to visualize trends. An example of how to interact with the program through the command line in shown in the fourth screenshot. In all of these scenarios, housing data was used to predict prices. </p>

              <p>The main code can be found at the below link: 
                <br> 
                <a target="_blank" href="https://github.com/atorshizi/Personal_Projects/blob/main/Regression">Source Code</a> 
              </p> 
            </details> 

            <details> 
              <summary>Score Calculator</summary>
              <p>This project draws on the Java swing toolkit to create a Java program and GUI that interacts with the user to get bowling score frame by frame for a standard game with 10 frames. It can then calculate the game score based on the rules of bowling in which spares double the next score and a strike doubles the next two. </p>

              <p float="left">
                <img src="Screenshots/Screenshot 2022-09-13 005317.png" alt="screenshot the starting state" style="width:325px;">
                <img src="Screenshots/Screenshot 2022-09-13 005719.png" alt="example score 1" style="width:325px;">
                <img src="Screenshots/Screenshot 2022-09-13 005414.png" alt="example score 2" style="width:325px;">
                <img src="Screenshots/Screenshot 2022-09-13 005620.png" alt="screenshot error due to inputted score" style="width:325px;">
              </p>
              
              <p>In the above screenshots we can see the state of the program when it first launched as well two example scores with the correct final scores shown based on the inputs, respectively. In the last image, we see the result of an incorrect/impossible score being inputted and the error message that is shown as a result - this is because of the score of 11 being inputted in the last frame which is not possible in a standard game of bowling. </p> 
            <p>The main code can be found at the below link: 
              <br> 
              <a target="_blank" href="https://github.com/atorshizi/Personal_Projects/blob/main/Score%20Finder">Source Code</a> 
            </p> 
          </details> 

          <details>
            <summary>TicTacToe Android App</summary>
            <p>This folder contains all of the files used to create an Android app of the classic TicTacToe game; the app shows which player's turn it is and can also determine once a winner/tie has been achieved. </b>
            <p float="left">
              <img src="Screenshots/Screenshot_20220908-104452_TicTacToe.png" alt="screenshot of app first loaded" style="width:150px;">
              <img src="Screenshots/Screenshot_20220908-104507_TicTacToe.png" alt="screenshot of mid game play" style="width:150px;">
              <img src="Screenshots/Screenshot_20220908-104524_TicTacToe.png" alt="screenshot of a finished game" style="width:150px;">
              <img src="Screenshots/Screenshot_20220908-104536_TicTacToe.png" alt="screenshot of a finished game" style="width:150px;">
              <img src="Screenshots/Screenshot_20220908-104611_TicTacToe.png" alt="screenshot of a finished game" style="width:150px;">
             </p>
            <p>The images above show a demonstration of the game when it is first started as well as different scenarios in which either player X or player O win the game or if the game ends in a tie. </p>             
            <p>The main code and apk can be found at the below links: 
              <br> 
              <a target="_blank" href="https://github.com/atorshizi/Personal_Projects/blob/main/TicTacToe/app/src/main/java/com/example/TicTacToe/MainActivity.java">MainActivity.java</a> 
              <br> 
              <a target="_blank" href="https://github.com/atorshizi/Personal_Projects/blob/main/TicTacToe/app/src/main/res/layout/activity_main.xml">MainActivity.xml</a> 
              <br> 
              <a target="_blank" href="https://github.com/atorshizi/Personal_Projects/blob/main/TicTacToe/app/TicTacToe.apk">apk file</a> 
            </p> 
          </details> 
        </div>
    </main>
</body>
</html>
